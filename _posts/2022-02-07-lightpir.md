---
layout: post
title: LightPIR. Privacy-Preserving Route Discovery for Lightning (paper summary)
published: false
---

<!-- CHANGE PUBLISHED TO FALSE TO PUSH AS DRAFT -->
<!-- CHANGE DATE BEFORE PUBLICATION -->

Lightning is currently source-routed.
This means that each sender does a local route search on the full network graph.
This may become unsustainable as Lightning grows grows.
Naively outsourcing route discovery to dedicated servers harms privacy: the servers know who is paying whom.
The [LightPIR paper](https://arxiv.org/abs/2104.04293) proposes a solution.
The authors combine private information retrieval with all-pairs-shortest-path pre-computation with hub labeling, optimized for real LN topology.
In this post, I summarize the LightPIR protocol and outline future challenges to turn it from a research prototype to a real-world implementation.


# Lightning routing

Multi-hop routing is a key feature of Lightning.
Payments between users with no shared channels allows Lightning to truly perform as a network: every (public) channel provides liquidity not only its counterparts but also to everyone else.
Currently, Alice (the sender) defines the entire route to Bob (the receiver).
To do so, Alice stores a snapshot of the public network based on P2P gossip.

As LN grows, it may become infeasible for each node to store a full network snapshot.
Although the current LN size (18410 nodes and 82128 channels at the time of this writing, as per [ACINQ](https://explorer.acinq.co/)) doesn't sound like a lot by modern hardware standards, if we expect Lightning to grow my orders of magnitude, source routing may become challenging.
Indeed, we want Lightning to work even on an IoT device with a tiny battery, little storage, and sporadic Internet connection.
This justifies the need for alternative efficient route discovery algorithms.


# A naive trust-based solution

Let's start with a naive trust-based solution and then improve it in both efficiency and privacy.
Imagine we have specialized nodes (_servers_) to do route discovery.
Generally, we can evaluate a client-server protocol using many metrics:

- server-side computation;
- server-side storage;
- client-side computation;
- client-side storage;
- communication.

There are two approaches to server-based route discovery:
1. a server calculates a route from Alice to Bob when Alice asks for it (just-in-time);
2. a server _pre-calculates_ shortest routes for _each_ pair of nodes and looks up the appropriate route when requested. This approach is called "**all pairs shortest paths**", or **APSP**.

The just-in-time approach is less demanding but may introduce higher latency for the client.
APSP requires more computation and storage from the server.
In a practical solution, a server would likely pre-calculate some routes and compute others routes just-in-time.

The protocols we've considered so far are not private.
The server knows what route a client requests (that is, who is paying whom).
In contrast, the current source-based route discovery is relatively private[[^1]].
Our goal is to come up with a protocol that would combine the efficiency of APSP approaches with the privacy of client-side route discovery.

[^1]: Privacy attacks exist with source-based routing too, i.e. using [timing](https://arxiv.org/abs/2006.12143), [cross-layer deanonymization](https://arxiv.org/abs/2007.00764), or [balance probing](https://s-tikhomirov.github.io/lightning-probing-2/), but at least no server learns right away which route the sender has chosen.


# Private information retrieval (addressing privacy)

Let's put LN specifics aside for a moment.
Consider a server that stores a database `DB`, which is an array of boolean values.
The client wants to obtain the `i`-th array element.
A straightforward protocol would be: the client sends `i` to the server, and the server replies with `DB[i]`.
Clearly, the server knows which element the client requests.

Can a client obtain `DB[i]` _without the server knowing_ `i`?
Theoretically, the only solution is to send the whole database to the client.
This is hardly practical.
Instead of the problem we know is practically unsolvable, let's bend the rules a bit and consider a related problem with more security assumptions.
Here is where **private information retrieval** (PIR) comes in.

There are two branches of PIR: computational (CPIR) and information-theoretical (IT-PIR).
CPIR allows for PIR with a single server but is slower and involves advanced cryptography (keyword: fully homomorphic encryption).
IT-PIR, on the other hand, is fast, uses cheap cryptography, but only works with _multiple_ servers.
LightPIR is based on IT-PIR, so we'll use the term PIR to refer to IT-PIR unless stated otherwise.

Consider _two_ servers holding identical database copies.
A simple PIR protocol works under an additional security assumption that **the servers don't collude**;

1. the client generates a random string `r` and another string `r'` that only differs from `r` in the `i`-th bit;
1. the client sends `r` to the first server;
1. the first server selects elements at indices `j` where `r[j]=1` and responds with a single bit `d` that is a XOR of those elements;
1. the client sends `r'` to the second server;
1. the second server responds analogously with a single bit `d'`;
1. the client locally computes `d XOR d' = D[i]`.

In other words, the client obtains two bits that are XOR's of _nearly identical_ subsets of the database elements.
The only difference between those subsets is that one includes `D[i]` and the other doesn't.
XOR'ing them together, the client discovers `D[i]` locally.

This simple PIR algorithm is not very efficient.
Server-side computation cost and the communication cost are linear in the database size.
Even though each response is a single bit long, the _request_ `r` is as long as the database itself.

Advanced PIR protocols are more efficient.
One idea is to consider the database as a two-dimensional matrix instead of a linear array.
This allows to improve communication complexity by increasing the number of servers.
For instance, with four non-colluding servers, the client only sends request of length of _square root_ of the number of elements.
For more details, see [this introductory lecture](https://www.youtube.com/watch?v=HFFPeYrz3ak).


# Hub labeling (addressing efficiency)

Any digital map service finds a route across a continent in seconds.
Isn't it amazing, considering that such a road graph may contain millions of points?
For instance, the widely used [Dijkstra's algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm) is linear(-ish) in the graph size.
Turns out, there is a lot of space for optimizations in route discovery algorithms.
For practical applications, our main concern is the performance on _real_ graphs (such as road networks), not the asymptotical complexity.
Therefore, we can exploit the properties of those particular graphs.

Consider a road network with millions of intersections.
Those intersections not equally likely to occur on a random route.
Highways comprise a small minority of roads but play a hugely important role in long-distance travel.
(That's the point of a highway system.)

Most routes (except very short ones) follow this pattern:
1. drive to the nearest highway entrance;
1. drive along the highway;
1. exit the highway and drive to your destination.

With that in mind, route discovery algorithms have been heavily optimized.
A key optimization is called **hub labeling**.
Informally, a hub is a node that is part of _many_ _sufficiently long_ _shortest_ paths.
(Each highlighted word in the previous sentence should be and of course is formally defined.)
Remember an earlier APSP method, where the server pre-computes routes for each pair of nodes?
Let's now modify APSP with hubs in mind.

Let the server store, for each node, a list of its hubs and the shortest route to each hub.
Finding a route from Alice to Bob would go like this:

1. find a node that both Alice and Bob consider their hub;
1. look up the shortest route from Alice to the hub;
1. look up the shortest route from the hub to Bob;
1. concatenate the two pieces and get the full route.

An attentive reader might spot an issue here.
What happens if there is no common hub between Alice and Bob?
To avoid this, hub sets are chosen to **intersect all shortest paths** of non-trivial length.
This problem is equivalent to the [set cover problem](https://en.wikipedia.org/wiki/Set_cover_problem), which is NP-complete.
Heuristic algorithms do the job well enough for practical applications.

A (heuristic) hub set labeling algorithm computes shortest paths between any pair of nodes and assign hub labels.
The hub sets generated by a heuristic are valid (i.e., they allow for lookup-and-concatenate route discovery) but may be suboptimal (include more hubs than necessary).
Still, the result is good enough for practical applications.


# LightPIR: putting this all together

At long last, let's discuss the contributions of the LightPIR paper!
The authors combine PIR and hub labeling (HL) for route discovery in payment channel networks (such as Lightning).
Assuming three types of nodes (a content provider, servers, and clients), the protocol goes as follows:

1. the content provider creates a network snapshot and sends its copies to the servers;
1. each server performs hub labeling on the snapshot;
1. clients query multiple servers using a PIR protocol.

The novelty of the paper is an improved HL algorithm tailored to the LN topology.
Intuitively, the LN is _even more_ centralized than highway networks, which can be effectively exploited.
The LN-optimized HL algorithm goes like this:

1. select `K` nodes with the highest degree, put them in the hub set for all nodes;
1. exclude those hubs from the graph;
1. find the remaining hubs based on the pruned graph.

This makes sense if we consider the de-facto hub-and-spoke structure of Lightning.
Prior HL algorithms would be repeatedly "discovering", for instance, that the ACINQ node (the most connected public node at the time of this writing) belongs to the hub set for Alice, and for Bob, and for Carol...
The proposed algorithm instead simply adds ACINQ to all hub sets.
The same applies to `K` most connected nodes, which are then temporarily excluded from the graph.
Additional "node-specific" hubs are then discovered on the pruned graph, which is faster.
How many top-connected nodes shall we pre-select into all hub sets?
The authors run experiments on historic LN snapshots and conclude that `K` should be around 100.


## A note on dataset validity

Initially, I had some doubts about the dataset the paper is based on.
The data source is Christian Decker's [Lightning Network Gossip](https://github.com/lnresearch/topology) repository, which is trustworthy.
However, the following sentence raised suspicion:

> although the network size almost doubles from March 2019 to January 2021, the number of nodes in the largest SCC remains pretty consistent across all snapshots (≈ 2500 nodes)

This seemingly contradicted other sources (like [Bitcoin Visuals](https://bitcoinvisuals.com/lightning)) that report a consistent growth in the number of Lightning nodes during that period.
Moreover, in the snapshots I took for [my own research](https://s-tikhomirov.github.io/lightning-probing-2/) in 2021, 99% of nodes belonged to the main connected component.

After all, there is no contradiction.
The LightPIR paper uses a directed graph model and considers the main _strongly_ connected component.
The main SCC is the largest set of nodes such that there is a _directed_ route between each pair of them.
It may be true that the main connected component comprises nearly all nodes, while the main _strongly_ connected component is much smaller.
This is possible, for instance, if there are many leaf nodes with uni-directional channels.
I would assume that end-users' wallets may exhibit such a pattern, but I'm not sure such wallets advertise their existence at all, meaning they may not be part of the public graph.
In short, more thought is required to more accurately reflect the real-world network structure of the LN in routing models.

Also, I should not that it's not at all surprising that the optimal number `K` is nearly constant throughout the time period in question.
Running the same algorithm on nearly identical datasets produces nearly identical results.
Would the results change if we consider the main _non-strongly_ connected component?


# Implementation prospects

In this section, I'll share my thoughts on the potential of LightPIR to be implemented in the Lightning network.
Assuming LightPIR is a good idea in principle, how should it be adapted for a more realistic network model?

## Non-collusion assumption

PIR protocols are based on the assumption that servers don't collude.
The first paper PIR ([Chor et al., 1995](https://madhu.seas.harvard.edu/papers/1995/pir-journ.pdf)) justifies the non-collusion assumption from the reputational standpoint:

> We assume that the servers do not collude in trying to violate the user's privacy. <...> A detected violation of the privacy guarantees will result in severe damage to the server. It is as if a bank were caught in fraud. <...> In the rare case where the user values its potential loss as more substantial than the server's risk, the user should not use a PIR scheme in which privacy depends on a noncollusion assumption.

This seemingly contradicts the permissionless nature of Lightning.
Yet, Lightning nodes already have a somewhat persistent identity and reputation.
This is especially relevant for Lightning Service Providers, or LSPs (a vague term for large nodes that provide LN services professionally).
If LSPs put their reputation on the line and are punished for colluding, the scheme might work in practice.

The key question then is: how to provably detect collusion?

## IT-PIR vs CPIR

As mentioned earlier, there are two kinds of PIR: information-theoretical (IT-PIR) and computational (CPIR).
The IT-PIR approach, which LightPIR is based on, achieves exactly zero information leakage even if the attacker has infinite computing power.
CPIR, on the other hand, allows for a single-server PIR, assuming the attacker's resources are bounded.
The paper doesn't justify the choice of IT-PIR as opposed to against CPIR.

The aforementioned 1995 paper writes on the topic of non-collusion assumption:

> The single-server computational PIR scheme of [Kushilevitz and Ostrovsky (1997)](https://web.cs.ucla.edu/~rafail/PUBLIC/34.pdf) addresses this concern.

Wouldn't CPIR approach be more suitable for Lightning?
An upper bound on attacker's resources is the assumption under which most real-world systems, including Bitcoin and Lightning, operate anyway.
What are the downsides of CPIR in this context?

## A single source of network graph data

The LightPIR model assumes a single source of truth about network topology.
A dedicated entity (the _content provider_) compiles the graph and sends its copies to servers.

In contrast, Lightning has no canonical network view.
Each node composes its own graph based on gossip messages.
(However, everyone is very likely to be aware of the most active and connected nodes.)

There might be two ways to reconcile theory and practice.
First, we could amend the theory to allow database copies to be slightly different.
The servers then could independently compile their graphs based on gossip.
Second, LN nodes could opt-in for a centralized data source.
For instance, a well-known LSP (think 1ML or ACINQ) would periodically publish a fresh network snapshot.
Servers (maintained by, for instance, by wallet providers), would download the snapshot and announce to their clients that they provide PIR-based route discovery based on a snapshot from a certain date.
Clients would then query a random subset of wallet providers to privately retrieve routes.

## A single route quality metric

LightPIR assumes that all clients use the same metric to evaluate route quality.
In the simplest case (ignoring fees), the model provides _shortest_ routes.
If we assign edge costs proportional to fees, we'd be talking about _cheapest_ routes.

On the one hand, the cheapest routes model may capture the desires of many clients.
On the other hand, this assumption may open clients to attacks where the adversary [attracts payments by advertising low fees](https://arxiv.org/abs/1909.06890).

On top of that, different clients may have different route quality metrics.
Total fees may be just one of the components of route quality function.
Clients may also optimize for:

- route length (longer routes increase the chance of payment failure);
- success probability ([related to](https://arxiv.org/abs/2107.05322) but not fully defined by route length);
- avoiding specific nodes (known adversarial nodes, nodes controlled by a certain entity, nodes located in a certain region, etc).

Again, we may bridge this gap between theory and practice in one of two ways (or something in between):

1. amend the theory to allow for user-defined route quality metrics;
1. use the protocol as is for users who prefer a simple goal (such as minimizing fees).

## The model doesn't account for amounts and fees

The graph model as presented in the paper is quite simplified.
First, it doesn't account for payment amounts and channel capacities.
Second, it doesn't account for fees (though I'm not exactly sure[[^2]]).

[^2]: Section 2 introduces a directed graph with _weighted_ edges, but Section 7 states that the authors "do not yet take into account fees", and that their work is optimized for "sending a fixed amount of Bitcoin". Also, Section 4.2.2 says that the labeling algorithm "requires that the path weights are unique integers, but this is easily achieved with insignificant perturbations of edge weights (in our case we initially set the weights to be the channel base fees and then perturbed them)". It's unclear to me whether edge weights are considered after all, and if so, whether weights equal fees. Even if _base_ fees are considered, an accurate fee model must also account for the component of LN fees that is _proportional_ to the payment amount, which means accounting for amounts, capacities, and balances.

The [key challenge](https://rusty-lightning.medium.com/lightning-routing-rough-background-dbac930abbad) in scaling LN routing is accommodating _dynamic_ parts of the graph.
The nodes and channels are somewhat static, while fee and policy updates may be frequent.
How should LightPIR be modified to not only account for fees but also to reflect frequent fee updates?


# Conclusion

LightPIR is a route discovery protocol for payment channel networks that combines private information retrieval and hub labeling.
It builds on prior work in route discovery for road networks and achieves higher efficiency by exploiting LN's centralized topology.
Simulations based on historical LN snapshots indicate how to best parameterize LightPIR.

LightPIR exemplifies papers that apply prior research findings in new contexts (in our case, payment channel networks).
I think this approach is valuable and somewhat underappreciated.
I find is likely that there are lots of valuable ideas in scientific literature from long before Bitcoin came along that may be useful for current development.
Yet, at least in the case of LightPIR, more effort is required to turn it into an implementation-ready proposal.


# Further reading

There are a few other papers on routing in payment channel networks:

- [SilentWhispers](https://eprint.iacr.org/2016/1054) ([my summary](https://s-tikhomirov.github.io/silentwhispers/))
- [SpeedyMurmurs](https://arxiv.org/abs/1709.05748) ([my summary](https://s-tikhomirov.github.io/speedymurmurs/))
- [Spider](https://arxiv.org/abs/1809.05088) ([my summary](https://s-tikhomirov.github.io/spider-network/))
- [Flare](https://bitfury.com/content/downloads/whitepaper_flare_an_approach_to_routing_in_lightning_network_7_7_2016.pdf)

And some more practice-oriented proposals as well:

- [Rendez-vous routing](https://bitcoinops.org/en/topics/rendez-vous-routing/)
- [Trampoline payments](https://bitcoinops.org/en/topics/trampoline-payments/)

Reconciling these and probably other ideas into a unified practical protocol for Lightning routing remains a direction for future work.
